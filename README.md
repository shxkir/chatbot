# PDF RAG Chatbot - Complete Implementation

A production-ready PDF chatbot with **Retrieval-Augmented Generation (RAG)** using LangChain, OpenAI, and Pinecone.

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))
- Pinecone account ([Sign up here](https://www.pinecone.io/))

### Setup in 3 Steps

**1. Install Dependencies**
```bash
npm install
```

**2. Configure Environment**

Edit `backend/.env.local`:
```env
OPENAI_API_KEY=sk-proj-your-key-here
PINECONE_API_KEY=pcsk-your-key-here
PINECONE_INDEX_NAME=pdf-chatbot
```

Create a Pinecone index:
- Name: `pdf-chatbot`
- Dimensions: `1536`
- Metric: `cosine`

**3. Start Servers**
```bash
npm run dev
```

**4. Open Browser**
- LangChain version: http://localhost:3001/langchain-upload
- Original version: http://localhost:3001/upload

## ğŸ¯ Two Implementations

This project includes **two working implementations**:

### LangChain Implementation â­ (Recommended)

**Routes**:
- `POST /api/langchain/upload` - Upload & process PDF
- `POST /api/langchain/query` - Query with RAG

**Features**:
- âœ… RecursiveCharacterTextSplitter for smart chunking
- âœ… Automatic embedding and storage
- âœ… Production-ready abstractions
- âœ… Prompt templates
- âœ… Better context preservation

**Pages**:
- `/langchain-upload` - Upload interface
- `/langchain-chat` - Chat interface

### Original Implementation

**Routes**:
- `POST /api/upload` - Basic upload
- `POST /api/query` - Basic query

**Features**:
- âœ… Simple, direct implementation
- âœ… Minimal dependencies
- âœ… Easy to understand
- âœ… Great for learning

**Pages**:
- `/upload` - Upload interface
- `/chat` - Chat interface

Both implementations work simultaneously and independently!

## ğŸ“ Project Structure

```
chatbot/
â”œâ”€â”€ backend/                          # Express + LangChain backend
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ langchain.js             # LangChain RAG implementation â­
â”‚   â”‚   â”œâ”€â”€ embeddings.js            # Original OpenAI service
â”‚   â”‚   â”œâ”€â”€ pinecone.js              # Original Pinecone service
â”‚   â”‚   â””â”€â”€ pdf.js                   # PDF utilities
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ langchain-upload.js      # LangChain upload endpoint â­
â”‚   â”‚   â”œâ”€â”€ langchain-query.js       # LangChain query endpoint â­
â”‚   â”‚   â”œâ”€â”€ upload.js                # Original upload
â”‚   â”‚   â””â”€â”€ chat.js                  # Original query
â”‚   â”œâ”€â”€ server.js                     # Express server
â”‚   â””â”€â”€ .env.local                    # Your API keys
â”‚
â”œâ”€â”€ frontend/                         # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ langchain-upload/    # LangChain upload page â­
â”‚   â”‚   â”‚   â”œâ”€â”€ langchain-chat/      # LangChain chat page â­
â”‚   â”‚   â”‚   â””â”€â”€ (protected)/         # Original pages
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ LangChainUploader.tsx â­
â”‚   â”‚   â”‚   â”œâ”€â”€ LangChainChat.tsx    â­
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUploader.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ChatUI.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useLangChainChat.ts  â­
â”‚   â”‚   â”‚   â””â”€â”€ useChat.ts
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â”œâ”€â”€ langchainApi.ts      â­
â”‚   â”‚       â””â”€â”€ backendApi.ts
â”‚   â””â”€â”€ .env.local
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ START_HERE.md                 # ğŸ‘ˆ Start with this!
    â”œâ”€â”€ LANGCHAIN_README.md           # LangChain overview
    â”œâ”€â”€ LANGCHAIN_GUIDE.md            # Comprehensive guide
    â”œâ”€â”€ ARCHITECTURE.md               # System architecture
    â”œâ”€â”€ IMPLEMENTATION_COMPARISON.md  # Compare both versions
    â””â”€â”€ QUICK_START.md                # Quick reference

â­ = New LangChain files
```

## ğŸ”§ How It Works

### Upload Flow (LangChain)
```
1. User uploads PDF
   â†“
2. Extract text with pdf-parse
   â†“
3. Split with RecursiveCharacterTextSplitter (1500 chars, 200 overlap)
   â†“
4. Create embeddings with OpenAI (text-embedding-3-small, 1536 dims)
   â†“
5. Store in Pinecone with namespace pdf_<timestamp>
   â†“
6. Return namespace to frontend
```

### Query Flow (LangChain RAG)
```
1. User asks question
   â†“
2. Embed question with OpenAI
   â†“
3. Similarity search in Pinecone (top 5 chunks)
   â†“
4. Build context from retrieved chunks
   â†“
5. Generate answer with ChatGPT (gpt-4o-mini)
   â†“
6. Return answer + references with scores
```

## ğŸ¨ Features

### Backend
- âœ… PDF upload and text extraction
- âœ… Smart text chunking with LangChain
- âœ… OpenAI embeddings (text-embedding-3-small)
- âœ… Pinecone vector storage with namespaces
- âœ… Similarity search with scores
- âœ… RAG pipeline with ChatGPT
- âœ… CORS enabled for localhost:3001
- âœ… Error handling and validation
- âœ… Environment variable configuration

### Frontend
- âœ… Modern Next.js with TypeScript
- âœ… Drag-and-drop PDF upload
- âœ… ChatGPT-style interface
- âœ… Real-time status updates
- âœ… Reference display with relevance scores
- âœ… Loading states and animations
- âœ… Responsive design
- âœ… Error handling

## ğŸ§ª Testing

### Test Backend Health
```bash
curl http://localhost:5001/health
# Expected: {"status":"ok"}
```

### Test LangChain Upload
```bash
curl -X POST http://localhost:5001/api/langchain/upload \
  -F "file=@sample.pdf"
```

### Test LangChain Query
```bash
curl -X POST http://localhost:5001/api/langchain/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is this document about?",
    "namespace": "pdf_1699564123456"
  }'
```

## ğŸ“š Technology Stack

### Backend
- **Express** - Web server
- **LangChain** - RAG framework
  - RecursiveCharacterTextSplitter
  - PineconeStore integration
  - PromptTemplate
- **OpenAI** - Embeddings + ChatGPT
- **Pinecone** - Vector database
- **pdf-parse** - PDF text extraction
- **Multer** - File uploads

### Frontend
- **Next.js 14** - React framework
- **TypeScript** - Type safety
- **Axios** - HTTP client
- **CSS-in-JS** - Styled components

## ğŸ”‘ Environment Variables

### Backend (`backend/.env.local`)
```env
# Required
OPENAI_API_KEY=sk-proj-xxxxx
PINECONE_API_KEY=pcsk-xxxxx
PINECONE_INDEX_NAME=pdf-chatbot

# Optional (with defaults)
PORT=5001
FRONTEND_URL=http://localhost:3001
OPENAI_EMBED_MODEL=text-embedding-3-small
OPENAI_CHAT_MODEL=gpt-4o-mini
CHUNK_SIZE=1500
CHUNK_OVERLAP=200
RETRIEVAL_TOP_K=5
```

### Frontend (`frontend/.env.local`)
```env
NEXT_PUBLIC_API_URL=http://localhost:5001
```

## ğŸ“– Documentation

| Document | Purpose |
|----------|---------|
| **START_HERE.md** | Quick start guide (read this first!) |
| **LANGCHAIN_README.md** | LangChain implementation overview |
| **LANGCHAIN_GUIDE.md** | Comprehensive guide with examples |
| **ARCHITECTURE.md** | System architecture diagrams |
| **IMPLEMENTATION_COMPARISON.md** | Compare LangChain vs Original |
| **QUICK_START.md** | Quick reference card |
| **SETUP_GUIDE.md** | Detailed setup instructions |

## ğŸ’¡ Commands

```bash
# Install all dependencies
npm install

# Start both servers (recommended)
npm run dev

# Start servers separately
npm run dev:backend   # Backend on port 5001
npm run dev:frontend  # Frontend on port 3001

# Install backend only
npm run install:backend

# Install frontend only
npm run install:frontend
```

## ğŸ› Troubleshooting

### "Network error contacting /api"
- Check backend is running: `npm run dev:backend`
- Verify port 5001 is free: `lsof -i :5001`
- Check `NEXT_PUBLIC_API_URL` in `frontend/.env.local`

### "OPENAI_API_KEY is not set"
- Add your API key to `backend/.env.local`
- Restart the backend server

### "Namespace not found"
- Upload a PDF first at `/langchain-upload`
- Use the returned namespace in queries

### CORS errors
- Check `FRONTEND_URL` in `backend/.env.local`
- Should be `http://localhost:3001`

## ğŸ’° Cost Estimates

Per 100-page PDF:
- **Embeddings**: ~$0.01 (one-time per document)
- **Queries**: ~$0.005 per question
- **Pinecone**: Free tier includes 100K vectors

Total: **~$0.50/month** for moderate usage

## ğŸš€ Production Deployment

### Backend (Railway, Render, or AWS)
1. Deploy backend to hosting platform
2. Set environment variables in platform dashboard
3. Update `FRONTEND_URL` to production frontend URL

### Frontend (Vercel or Netlify)
1. Deploy frontend to hosting platform
2. Set `NEXT_PUBLIC_API_URL` to production backend URL
3. Ensure it's a build-time environment variable

See **LANGCHAIN_GUIDE.md** for detailed deployment instructions.

## ğŸ“ Learning Path

1. **Quick Start**: Follow setup steps above (5 min)
2. **Try It**: Upload a PDF and chat (10 min)
3. **Explore**: Try both implementations (10 min)
4. **Compare**: Read IMPLEMENTATION_COMPARISON.md (5 min)
5. **Deep Dive**: Read LANGCHAIN_GUIDE.md (20 min)
6. **Customize**: Modify to fit your needs

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/api/langchain/upload` | POST | LangChain upload â­ |
| `/api/langchain/query` | POST | LangChain query â­ |
| `/api/upload` | POST | Original upload |
| `/api/query` | POST | Original query |

â­ = Recommended for new projects

## âœ¨ Key Differences

| Feature | Original | LangChain |
|---------|----------|-----------|
| **Text Splitting** | Simple chunking | RecursiveCharacterTextSplitter |
| **Code Complexity** | More verbose | More concise |
| **Abstraction** | Low-level | High-level |
| **Best For** | Learning | Production |
| **Namespace Format** | UUID | `pdf_<timestamp>` |

## ğŸ¯ Why LangChain?

- âœ… **Better Chunking**: RecursiveCharacterTextSplitter preserves context
- âœ… **Production Ready**: Battle-tested abstractions
- âœ… **Easier to Extend**: Add agents, chains, memory
- âœ… **Best Practices**: Industry-standard approach
- âœ… **Cleaner Code**: Less boilerplate

## ğŸ¤ Contributing

This is a complete, working implementation. Feel free to:
- Customize the UI
- Add new features (authentication, history, etc.)
- Deploy to production
- Use as a template for your projects

## ğŸ“ License

This project is provided as-is for educational and commercial use.

## ğŸ™ Acknowledgments

Built with:
- [LangChain](https://js.langchain.com/) - RAG framework
- [OpenAI](https://openai.com/) - Embeddings and chat
- [Pinecone](https://www.pinecone.io/) - Vector database
- [Next.js](https://nextjs.org/) - React framework
- [Express](https://expressjs.com/) - Web server

---

## ğŸ‰ Ready to Go!

Your complete PDF-RAG chatbot is ready. Just:

1. **Add API keys** to `backend/.env.local`
2. **Create Pinecone index** (1536 dimensions, cosine metric)
3. **Run** `npm run dev`
4. **Open** http://localhost:3001/langchain-upload
5. **Upload** a PDF and start chatting!

**Need help?** Check **START_HERE.md** for detailed instructions.

**Happy chatting! ğŸš€**
